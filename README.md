# ARIA data
In this repository, different kinds of data can be found. First, there is a dataset designed to fine-tune a Large Language Model (LLM) to generate automation rules for Home Assistant, given the description (in plain text) of smart home scenarios (including building elements, topology, devices deployed, etc.). Second, there is a set of tests conducted for various case studies to validate the LLM's ability to generate automation rules for Home Assistant and Answer Set Programming (ASP) systems through example-based learning (zero-shot and few-shot) learning techniques.

## Fine-tuning dataset
To do...

## Example-based learning: Cases of study
To test whether a LLM (GPT-4 concretely) can generate commonsense behavior for smart home systems through the definition of automation rules, a series of tests has been designed for various case studies. The case studies are as follows:

- **Study case A**: *The deployment covers three spaces. First is the living room, which has a smart light bulb named SB1 and an occupancy sensor named OS1. Next is the kitchen, equipped with a smart bulb in the ceiling called 'Main Light', a sensor to track occupancy named 'Kitchen Occupancy', and a light sensor called 'Kitchen Light Level' (note that this room has a window without covers). Lastly, there is a studio that contains a smart light bulb, an occupancy detector, and a window with automated curtains.*
- **Study case B.1**:
- **Study case B.2**:
- **Study case B.3**:

Considering that all scenarios have been crafted to test the LLM's ability to generate lighting automations, the uppercase letter in each case study's name signifies the type of lighting solution to be automated. For instance, the letter 'A' suggests that the challenge involves a standard lighting issue (such as turning on the lights when someone enters a room, if lights are present). The letter 'B' suggests more complex lighting challenges where typical light sources are not available (such as using the lights from a connected room to illuminate a space with non-functional lights).

The set of case studies can be found in the `studycases/` folder, organized as follows:

- `studycases/A.1/`: Contains the tests for Case Study A.1.
- `studycases/B.1/`: Contains the tests for Case Study B.1.
- `studycases/B.2/`: Contains the tests for Case Study B.2.
- `studycases/B.3/`: Contains the tests for Case Study B.3.

Inside each of these folders (`<usecase>`), the information is organized in the following manner:

- `<usecase>/README.md`: contains all the information about the study case, including the description of the scenario, the tests performed, and the results obtained.
- `<usecase>/scenario.<ext>`: Contains the floor plan of the scenario (the file may be split into different images if the scenario is too large), where `<ext>` can be any image file extension such as png, jpg, etc.
- `<usecase>/scenario-ineditor.json`: Contains the scenario description in JSON format, ready to be imported into the IndoorGML editor, InEditor.
- `<usecase>/scenario-indoorgml.gml`: Contains the IndoorGML description of the scenario.
- `<usecase>/scenario-ontologies.ttl`: Contains the scenario description in RDF format using the BOT (Building Topology Ontology), DICBM (Digital Construction - Building Materials), and SOSAc (SOSA Capabilities) ontologies.
- `<usecase>/tests/`: Contains the tests performed for the case study.

### Tests
For each case described above, tests were performed providing the LLM with the description of the scenario in two different formats: plain text (written as shown above) and a structured format. The aim is first to verify that the LLM can comprehend a scenario description in natural language. Specifically, it should understand the things that can be sensed (which state changes are detectable, and consequently, which situations can be recognized) and the actions that can be executed (the kinds of responses the intelligent environment can generate). Subsequently, the goal is to confirm that the LLM can interpret the scenario description just as effectively when presented in a structured format. This format standardizes the scenario's portrayal, eliminating the ambiguities of natural language and the user's variable ability to convey the setup in a manner the LLM can grasp.

Additionally, tests utilizing zero-shot and few-shot learning techniques were carried out by providing the LLM with: 1) only the scenario description for automation (zero-shot), 2) examples including the description and the desired rules to be generated for each case (few-shot), and 3) examples comprising the description, the desired rules, plus an explanation of the rules designed for the system in each case (few-shot with explanation).

It is also crucial to note that tests were designed to generate rules for both Home Assistant and Answer Set Programming (ASP) systems, with the latter enabling the validation of rules generated by the LLM.

Given this context, the tests performed for each case study can be found in the `<usecase>/tests/` folder, organized as follows:

- `ha/`: Contains the tests performed for Home Assistant rules.
- `asp/`: Contains the tests performed for ASP rules.

Within each of these directories (`<usecase>/tests/<system>/`), the information is structured as follows:

- `t-zeroshot.md` and `s-zeroshot.md`: Contain the tests performed using the zero-shot learning technique, with the LLM provided with the scenario description in plain text and structured format, respectively.
- `t-fewshot.md` and `s-fewshot.md`: Contain the tests performed using the few-shot learning technique, with the LLM provided with the scenario description in plain text and structured format, respectively.
  `t-fewshot-exp.md` and `s-fewshot-exp.md`: Contain the tests performed using the few-shot learning technique, giving an explanation of the rules generated for each example and with the scenario descriptions in plain text and structured format, respectively.

It is important to note that certain directories or files may not exist for some case studies.
